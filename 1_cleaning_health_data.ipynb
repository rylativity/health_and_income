{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE: You must unzip the 500 Cities Dataset first before reading it in with this line\n",
    "read_in = pd.read_csv(\"./datasets/500_Cities__Local_Data_for_Better_Health__2017_release.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering for census tract level data\n",
    "df = read_in[read_in['GeographicLevel'] == \"Census Tract\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.StateAbbr == \"DC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# these are the potential feature columns along with their counts\n",
    "# I need to pull the Data_Value along with the GeoLocation (as my unique identifier)\n",
    "df.Measure.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example of a Geolocation from the Data\n",
    "df.GeoLocation[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to use RegEx to creat latitude/longitude cols, example here\n",
    "re.findall(\"-\\d+.\\d+\", df.GeoLocation[58])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create latitude column for all rows\n",
    "df['Latitude'] = [float(re.findall(\"(\\d+.\\d+),\", df.GeoLocation[i])[0]) for i in df.index] # regex for lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create longitude column for all rows\n",
    "df['Longitude'] = [float(re.findall(\"-\\d+.\\d+\", df.GeoLocation[i])[0]) for i in df.index] # regex for long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to pull the data for which all locations have data\n",
    "measures_of_interest = df.Measure.value_counts().index[0:21] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# These are the variables/measures that we are interested in\n",
    "print(list(measures_of_interest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making a separate df for each measure\n",
    "df_list = [df[df[\"Measure\"] == x] for x in measures_of_interest] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little bit of reformatting of our dataframes, which will be merged together into a master df\n",
    "metric_dfs = []\n",
    "for df in df_list:\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    temp = pd.DataFrame({df.MeasureId[0]:df.Data_Value, # this is the actual value for the measure\n",
    "                         \"UniqueID\":df.UniqueID,\"geometry\":df.GeoLocation, # UniqueID and geolocation cols\n",
    "                         \"state\":df.StateAbbr, \"population\":df.PopulationCount,\n",
    "                         \"longitude\": df.Longitude, \"latitude\":df.Latitude,})\n",
    "    metric_dfs.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metric_dfs[0].UniqueID.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create master DF of all metrics dfs\n",
    "master = pd.merge(metric_dfs[0],metric_dfs[1], how='left',on=\"UniqueID\") \n",
    "for i in metric_dfs[2:]: # merging the rest with a for loop\n",
    "    master = master.merge(i, how = \"left\",on=\"UniqueID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "master.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate columns from merge\n",
    "master = master.T.drop_duplicates().T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns that have an _x suffix\n",
    "master.rename({\"geometry_x\":\"geometry\",\"state_x\":\"state\",\"population_x\":\"population\",\"longitude_x\":\"longitude\",\n",
    "              \"latitude_x\":\"latitude\"}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 5 digit fips codes can be pulled from the first 5 indices of the unique ID\n",
    "##... using this to aggregate data to the county level, since thats the income data I have\n",
    "master.UniqueID[0][0:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension to add county fips code column to all rows\n",
    "master[\"fips\"] = [x[0:5] for x in master.UniqueID] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check how much data we would lose by dropping columns containing NA values\n",
    "master.dropna().shape, master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since it's not too many, we can drop the rows without significantly affecting our analysis\n",
    "# Assumption made for simplicity that columns are missing entirely at random\n",
    "master.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping these columns since I am aggregating on fips code (County)\n",
    "clean_master = master.drop([\"UniqueID\", \"geometry\",\"population\",\"latitude\",\"longitude\",\"state\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_master.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need to change these columns from 'object' to 'float'\n",
    "for col in ['DIABETES', 'CHOLSCREEN', 'MHLTH', 'CSMOKING', 'BPHIGH', 'CANCER',\n",
    "       'CASTHMA', 'CHD', 'PHLTH', 'BPMED', 'KIDNEY', 'BINGE', 'DENTAL',\n",
    "       'STROKE', 'SLEEP', 'OBESITY', 'COPD', 'LPA', 'CHECKUP', 'HIGHCHOL',\n",
    "       'ARTHRITIS']:\n",
    "    clean_master[col] = clean_master[col].astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data by taking the means of each column and grouping by the \"fips\" code column\n",
    "agg_data = clean_master.groupby(\"fips\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# format for export so we dont lose leading 0s\n",
    "agg_data[\"fips\"] = agg_data['fips'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the data to csv\n",
    "agg_data.to_csv(\"./datasets/agg_county_data_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#484 resulting fips code values once aggregated\n",
    "agg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
